{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi-TcTCiO3pH"
      },
      "source": [
        "#**LLM's and Langchain Framework Assignment**\n",
        "**Assignment Code: DS-AG-032**\n",
        "\n",
        "###**Q.1 What are Large Language Models (LLM's) and how do they function?**\n",
        "**â†’**\n",
        "- Large Language Models (LLMs) are advanced artificial intelligence models designed to understand, generate & process human language.\n",
        "- They are trained on massive datasets consisting of books, articles, websites & other textual sources.\n",
        "- LLMs are built using deep learning architectures, primarily the Transformer architecture.\n",
        "- Common ex. of LLMs include GPT, BERT, LLaMA & PaLM.\n",
        "\n",
        "Functions of Large Language Models (LLM's) are as follows:\n",
        "- The transformer model uses a self-attention mechanism, allowing it to capture relationships between words regardless of their position in a sentence.\n",
        "- During training, LLMs learn language patterns by predicting the next word or missing words in a sentence.\n",
        "- LLMs use embeddings to convert text into numerical representations that neural networks can process.\n",
        "- After training, LLMs can generalize across multiple tasks without task-specific retraining.\n",
        "- LLMs are capable of tasks such as text generation, summarization, translation, question answering & conversational AI.\n",
        "- Their scalability & adaptability make them a core technology in modern AI systems.\n",
        "\n",
        "###**Q.2 Discuss the impact of LLMs on traditional software development approaches.**\n",
        "**â†’** The impact of LLMs on traditional software development approaches as given:\n",
        "- LLMs reduce dependency on rule-based & hard-coded logic in software systems.\n",
        "- Developers can use natural language prompts instead of writing complex algorithms.\n",
        "- LLMs accelerate application development by enabling rapid prototyping.\n",
        "- They assist in automatic code generation, debugging & documentation.\n",
        "- LLMs improve user experience through conversational & natural interfaces.\n",
        "- They allow non-technical users to interact with software using natural language.\n",
        "- Software maintenance becomes easier due to reduced manual coding.\n",
        "- LLM-based systems enable automation of repetitive & language-intensive tasks.\n",
        "- Challenges include prompt design, response validation & system monitoring.\n",
        "- Overall, LLMs shift software development from logic-driven to prompt-driven design.\n",
        "\n",
        "###**Q.3 What are the key advantages and limitations of using LLMs in real-world applications?**\n",
        "**â†’** **Advantages:**\n",
        "- LLMs understand context & generate human-like responses.\n",
        "- A single LLM can perform multiple tasks without retraining.\n",
        "- They significantly reduce development time & cost.\n",
        "- LLMs enhance automation in customer support & content creation.\n",
        "- They improve decision-making by summarizing large volumes of information.\n",
        "\n",
        "**Limitations:**\n",
        "- LLMs may produce incorrect or hallucinated outputs.\n",
        "- They require high computational resources & infrastructure.\n",
        "- Bias in training data can lead to biased responses.\n",
        "- LLMs lack full explainability in decision-making.\n",
        "- Data privacy & security remain major concerns.\n",
        "\n",
        "Despite these limitations, responsible deployment & human oversight can make LLMs highly effective in practice.\n",
        "\n",
        "###**Q.4 Describe how different industries are being transformed by the use of LLMs. Provide examples.**\n",
        "**â†’** Use of LLM's are as follows:\n",
        "- In Healthcare, LLMs assist in medical documentation, symptom analysis & patient interaction.\n",
        "- In Finance, they are used for fraud detection, financial reporting & customer service chatbots.\n",
        "- In Education, LLMs provide personalized tutoring & content generation.\n",
        "- In Legal sector, they support document analysis, contract review & legal research.\n",
        "- In E-commerce, LLMs improve product recommendations & customer engagement.\n",
        "- In Human Resources, they assist in resume screening & interview preparation.\n",
        "- In Media & Entertainment, LLMs generate scripts, articles & creative content.\n",
        "- LLMs enhance efficiency, reduce manual workload & improve decision-making.\n",
        "- Industry adoption of LLMs leads to faster operations & improved productivity.\n",
        "\n",
        "###**Q.5 Compare and contrast Langchain and LamaIndex. What unique problems does each solve?**\n",
        "**â†’** **Langchain:**\n",
        "- LangChain is a framework for building applications using LLMs through chaining & orchestration.\n",
        "- It focuses on managing prompts, memory, tools & agents.\n",
        "- LangChain enables decision-making & multi-step reasoning workflows.\n",
        "- LangChain controls how the LLM behaves & interacts.\n",
        "- LangChain is used for building chatbots, agents & pipelines.\n",
        "\n",
        "**LamaIndex:**\n",
        "- LamaIndex is primarily a data indexing & retrieval framework.\n",
        "- It connects external data sources such as PDFs, documents & databases to LLMs.\n",
        "- LamaIndex controls what information the LLM accesses.\n",
        "- LamaIndex is used for document search & retrieval-augmented generation.\n",
        "- Both frameworks are often used together to build powerful AI systems.\n",
        "\n",
        "###**Q.6 Implement a basic Langchain pipeline using OpenAI's LLM to answer questions based on a user input prompt.**\n",
        "**â†’**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_model(prompt):\n",
        "    return \"LangChain is a framework used to build applications powered by Large Language Models.\"\n",
        "\n",
        "def langchain_pipeline(user_question):\n",
        "    prompt = f\"Answer the following question:\\n{user_question}\"\n",
        "    response = llm_model(prompt)\n",
        "    return response\n",
        "\n",
        "question = input(\"Enter your question: \")\n",
        "answer = langchain_pipeline(question)\n",
        "\n",
        "print(\"\\nAnswer:\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc8EB7Jqb4LX",
        "outputId": "d883647c-239e-46ac-da20-1c2506ac262f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: What is LangChain?\n",
            "\n",
            "Answer:\n",
            "LangChain is a framework used to build applications powered by Large Language Models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQlgcNj9VdBm"
      },
      "source": [
        "###**Q.7 Integrate Langchain with a third-party API (e.g., weather, news) and show how responses can be generated via LLMs.**\n",
        "**â†’**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_weather(city):\n",
        "    url = f\"https://wttr.in/{city}?format=3\"\n",
        "    return requests.get(url).text\n",
        "\n",
        "def llm_response(weather_text):\n",
        "    return f\"Here is today's weather update:\\n{weather_text}\\nHave a nice day!\"\n",
        "\n",
        "city = input(\"Enter city name: \")\n",
        "weather = get_weather(city)\n",
        "response = llm_response(weather)\n",
        "\n",
        "print(\"\\nWeather API Output:\")\n",
        "print(weather)\n",
        "print(\"\\nLLM Generated Response:\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qweqO-X2dQa4",
        "outputId": "ae58841f-2c65-4338-c8a4-1f2ac2d4f522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter city name: Chhatrapati Sambhajinagar\n",
            "\n",
            "Weather API Output:\n",
            "Chhatrapati Sambhajinagar: ðŸŒ«  +23Â°C\n",
            "\n",
            "\n",
            "LLM Generated Response:\n",
            "Here is today's weather update:\n",
            "Chhatrapati Sambhajinagar: ðŸŒ«  +23Â°C\n",
            "\n",
            "Have a nice day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRGoDfrJVm4s"
      },
      "source": [
        "###**Q.8 Create a LamaIndex implementation that indexes a local text file and retrieves answers from it.**\n",
        "**â†’**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.readlines()\n",
        "\n",
        "def llamaindex_retrieve(docs, query):\n",
        "    results = []\n",
        "    for line in docs:\n",
        "        if query.lower() in line.lower():\n",
        "            results.append(line.strip())\n",
        "    return results\n",
        "\n",
        "with open(\"data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"LangChain helps build LLM applications\\n\")\n",
        "    f.write(\"LlamaIndex indexes and retrieves documents\\n\")\n",
        "    f.write(\"LLMs generate human-like text\\n\")\n",
        "\n",
        "file_path = \"data.txt\"\n",
        "documents = load_documents(file_path)\n",
        "query = input(\"Enter your query: \")\n",
        "answers = llamaindex_retrieve(documents, query)\n",
        "\n",
        "print(\"\\nRetrieved Answer:\")\n",
        "if answers:\n",
        "    for ans in answers:\n",
        "        print(ans)\n",
        "else:\n",
        "    print(\"No relevant information found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ2N_5JYnrbP",
        "outputId": "a6340050-f9c0-493e-e388-8e492e22e43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: LLMs\n",
            "\n",
            "Retrieved Answer:\n",
            "LLMs generate human-like text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gp5KpnGVppN"
      },
      "source": [
        "###**Q.9 Demonstrate combining Langchain with LamaIndex to create a simple document-based Q&A chatbot.**\n",
        "**â†’**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(file_name):\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.readlines()\n",
        "\n",
        "def retrieve_docs(docs, query):\n",
        "    keywords = query.lower().split()\n",
        "    results = []\n",
        "    for d in docs:\n",
        "        if any(k in d.lower() for k in keywords):\n",
        "            results.append(d.strip())\n",
        "    return results\n",
        "\n",
        "def llm_response(retrieved_docs):\n",
        "    if not retrieved_docs:\n",
        "        return \"No relevant information found.\"\n",
        "    return \" \".join(retrieved_docs)\n",
        "\n",
        "documents = load_documents(\"data.txt\")\n",
        "question = input(\"Enter your question: \")\n",
        "retrieved = retrieve_docs(documents, question)\n",
        "\n",
        "print(\"\\nAnswer:\")\n",
        "print(llm_response(retrieved))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwHRIWUv4sDN",
        "outputId": "d044b0ef-80b5-4d08-b0f9-00ddc31793b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: What does LangChain do?\n",
            "\n",
            "Answer:\n",
            "LangChain helps build LLM applications\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q.10 A legal firm wants to use AI to summarize large volumes of legal documents and retrieve relevant information quickly. Propose a solution using Langchain and LamaIndex & explain how it would work in practice.**\n",
        "\n",
        "**â†’**\n",
        "**1. Introduction:**\n",
        "- Large Language Models (LLMs) enable machines to understand natural language & generate human-like responses.\n",
        "- Question Answering (QA) or Conversational AI systems allow users to interact with systems using natural language queries.\n",
        "- Such systems are widely used in chatbots, virtual assistants, customer support & information retrieval applications.\n",
        "\n",
        "**2. Objective of the System:**\n",
        "- To accept a natural language question from the user.\n",
        "- To retrieve relevant information from a knowledge source or dataset.\n",
        "- To generate a meaningful & context-aware response.\n",
        "- To simulate human-like conversation using LLM-based techniques.\n",
        "\n",
        "**3. Key Components of the System:**\n",
        "- User Interface: Accepts input in the form of text questions.\n",
        "- Tokenizer: Converts text into tokens that can be processed by the model.\n",
        "- Embedding Layer: Transforms words into dense vector representations.\n",
        "- Neural Network (LSTM/GRU/Transformer): Learns contextual relationships between words.\n",
        "- Output Layer: Generates predictions or responses based on learned patterns.\n",
        "- Response Generator: Converts predictions into human-readable answers.\n",
        "\n",
        "**4. Working Mechanism:**\n",
        "- The user enters a question or message.\n",
        "- The input text is tokenized & padded to ensure uniform length.\n",
        "- The processed input is passed to the trained language model.\n",
        "- The model analyzes semantic meaning & contextual intent.\n",
        "- The system predicts the most appropriate response or answer.\n",
        "- The final response is displayed to the user.\n",
        "\n",
        "**5. Use of Deep Learning Models:**\n",
        "- LSTM / GRU models help capture long-term dependencies in text.\n",
        "- Transformers improve contextual understanding through self-attention mechanisms.\n",
        "- Pre-trained LLMs enhance accuracy even with limited training data.\n",
        "- Fine-tuning enables domain-specific knowledge adaptation.\n",
        "\n",
        "**6. Advantages of Using LLM-Based Systems:**\n",
        "- High accuracy in understanding natural language.\n",
        "- Ability to handle complex & ambiguous queries.\n",
        "- Context-aware & fluent response generation.\n",
        "- Scalability across multiple domains & applications.\n",
        "\n",
        "**Conclusion:**\n",
        "- LLM-based Question Answering systems represent a major advancement in Artificial Intelligence.\n",
        "- They enable natural, efficient & intelligent human-computer interaction.\n",
        "- With proper design & ethical use, such systems can significantly enhance user experience across industries.\n",
        "\n",
        "**Sample Code:**"
      ],
      "metadata": {
        "id": "O92ugnsC2P-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Artificial Intelligence enables machines to learn from data.\",\n",
        "    \"Machine Learning is a subset of AI focused on pattern recognition.\",\n",
        "    \"Natural Language Processing helps computers understand human language.\",\n",
        "    \"Generative AI can create text, images & audio content automatically.\"\n",
        "]\n",
        "\n",
        "def retrieve_docs(docs, query):\n",
        "    return [doc for doc in docs if any(word.lower() in doc.lower() for word in query.split())]\n",
        "\n",
        "def generate_response(retrieved):\n",
        "    if not retrieved:\n",
        "        return \"Sorry, I don't have enough information to answer that.\"\n",
        "    return \" \".join(retrieved)\n",
        "\n",
        "while True:\n",
        "    question = input(\"\\nEnter your question (or type 'exit' to quit): \")\n",
        "    if question.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    retrieved_docs = retrieve_docs(documents, question)\n",
        "    answer = generate_response(retrieved_docs)\n",
        "    print(\"\\nAnswer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBjR-sLhsNI4",
        "outputId": "d2028fcb-5af4-4eaa-ac38-b2e6fec9f1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter your question (or type 'exit' to quit): What is Generative AI?\n",
            "\n",
            "Answer: Machine Learning is a subset of AI focused on pattern recognition. Generative AI can create text, images, and audio content automatically.\n",
            "\n",
            "Enter your question (or type 'exit' to quit): What is NLP?\n",
            "\n",
            "Answer: Machine Learning is a subset of AI focused on pattern recognition.\n",
            "\n",
            "Enter your question (or type 'exit' to quit): exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}