{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**CNN Architecture**\n",
        "**Assignment Code: DS-AG-021**\n",
        "\n",
        "##**Q.1 What is the role of filters and feature maps in Convolutional Neural Network (CNN)?**\n",
        "\n",
        "**→**\n",
        "- **Filters (kernels):** Small weight matrices applied to input images to detect specific features (edges, textures, shapes).\n",
        "\n",
        "- **Feature Maps:** The result of applying filters via convolution. They highlight spatial patterns in the data.\n",
        "\n",
        "- **Role:** Filters extract low-level to high-level features & feature maps represent these extracted features layer by layer.\n",
        "\n",
        "##**Q.2 Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?**\n",
        "\n",
        "**→**\n",
        "\n",
        "**Padding:**\n",
        "- Adding zeros around the input image.\n",
        "- Preserves spatial dimensions.\n",
        "- Helps capture edge features.\n",
        "- Types: Valid (no padding), Same (output size preserved).\n",
        "\n",
        "**Stride:**\n",
        "- Step size of filter movement.\n",
        "- Stride 1 → more detailed feature map.\n",
        "- Stride >1 → reduces output size, speeds up computation.\n",
        "\n",
        "**Formula (output dimension):** Output = (Input - Filter + 2*Padding)/Stride + 1\n",
        "\n",
        "\n",
        "##**Q.3 Define receptive field in the context of CNNs. Why is it important for deep architectures?**\n",
        "\n",
        "**→** Receptive Field in the context of CNN's is the specific region of the input image that a CNN neuron responds to.\n",
        "\n",
        "**Importance:**\n",
        "- Larger receptive field → captures more context.\n",
        "- In deep CNNs, receptive fields grow, enabling detection of complex patterns.\n",
        "- Critical for tasks like object detection & medical imaging where context matters.\n",
        "\n",
        "##**Q.4 Discuss how filter size and stride influence the number of parameters in a CNN.**\n",
        "\n",
        "**→**\n",
        "\n",
        "**Filter Size:**\n",
        "- Parameters per filter = (filter_height × filter_width × input_channels) + bias.\n",
        "- Larger filters → more parameters, more computation.\n",
        "- More weights (parameters).\n",
        "- **For Ex.:**\n",
        "1. 3×3 filter on 1 channel = 9 parameters.\n",
        "2. 5×5 filter on 1 channel = 25 parameters.\n",
        "\n",
        "**Stride:**\n",
        "- Stride affects feature map size, not the number of parameters directly.\n",
        "- Larger stride reduces number of output neurons, lowering computational cost.\n",
        "- Does not change parameters directly but affects number of computations.\n",
        "- Larger stride = fewer feature map activations → less computation.\n",
        "\n",
        "##**Q.5 Compare and contrast different CNN-based architectures like LeNet, AlexNet & VGG in terms of depth, filter sizes & performance.**\n",
        "\n",
        "**→**\n",
        "\n",
        "**LeNet (1998):**\n",
        "- Shallow (5-7 layers), 5×5 filters.\n",
        "- Used for digit recognition (MNIST).\n",
        "- Application: Handwritten digit recognition.\n",
        "\n",
        "**AlexNet (2012):**\n",
        "- 8 layers deep, 11×11, 5×5 & 3×3 filters.\n",
        "- Used ReLU, dropout, GPUs.\n",
        "- Won ImageNet 2012 competition.\n",
        "- Achieved ImageNet breakthrough.\n",
        "\n",
        "**VGG (2014):**\n",
        "- Very deep (16-19 layers).\n",
        "- Only 3×3 filters, uniform architecture.\n",
        "- High accuracy but heavy computation.\n",
        "- Deeper, more accurate but computationally heavy.\n",
        "\n",
        "##**Q.6 Using keras, build  & train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training & evaluation.**\n",
        "\n",
        "**→**"
      ],
      "metadata": {
        "id": "hi-TcTCiO3pH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5uzQrXpOy6P",
        "outputId": "cc9290c8-6e82-440f-b95e-9d23dc82d36f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 38ms/step - accuracy: 0.8991 - loss: 0.3478 - val_accuracy: 0.9791 - val_loss: 0.0680\n",
            "Epoch 2/3\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 36ms/step - accuracy: 0.9830 - loss: 0.0584 - val_accuracy: 0.9830 - val_loss: 0.0501\n",
            "Epoch 3/3\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.9889 - loss: 0.0370 - val_accuracy: 0.9868 - val_loss: 0.0421\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9830 - loss: 0.0537\n",
            "Test Accuracy: 0.9868000149726868\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1,28,28,1).astype(\"float32\")/255\n",
        "x_test = x_test.reshape(-1,28,28,1).astype(\"float32\")/255\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test,y_test))\n",
        "loss, acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q.7 Load and preprocess the CIFAR-10 dataset using Keras & create a CNN model to classify RGB images. Show your preprocessing and architecture.**\n",
        "\n",
        "**→**"
      ],
      "metadata": {
        "id": "iQlgcNj9VdBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test,y_test))\n",
        "\n",
        "loss, acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuHfMJsuV1It",
        "outputId": "1491371a-3ebf-4ff9-97c5-9cb9ed28eb7a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 86ms/step - accuracy: 0.3877 - loss: 1.6824 - val_accuracy: 0.5792 - val_loss: 1.2100\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 84ms/step - accuracy: 0.6058 - loss: 1.1296 - val_accuracy: 0.6338 - val_loss: 1.0523\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 81ms/step - accuracy: 0.6591 - loss: 0.9775 - val_accuracy: 0.6614 - val_loss: 0.9798\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 81ms/step - accuracy: 0.6976 - loss: 0.8715 - val_accuracy: 0.6696 - val_loss: 0.9585\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - accuracy: 0.7319 - loss: 0.7759 - val_accuracy: 0.6958 - val_loss: 0.8877\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7032 - loss: 0.8807\n",
            "Test Accuracy: 0.6958000063896179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q.8 Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop & accuracy evaluation.**\n",
        "\n",
        "**→**"
      ],
      "metadata": {
        "id": "XRGoDfrJVm4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "train_loader = DataLoader(datasets.MNIST('.', train=True, download=True, transform=transform), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(datasets.MNIST('.', train=False, transform=transform), batch_size=1000)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(32*13*13, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 32*13*13)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(2):\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "print(\"Test Accuracy:\", correct/len(test_loader.dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJXtpzIsXgrN",
        "outputId": "03e2d8e0-c7fc-4548-af70-ebb4471d33e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 487kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.46MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.99MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q.9 Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.**\n",
        "\n",
        "**→**"
      ],
      "metadata": {
        "id": "8gp5KpnGVppN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    \"dataset_path\", target_size=(64,64), batch_size=32, class_mode='binary', subset='training')\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    \"dataset_path\", target_size=(64,64), batch_size=32, class_mode='binary', subset='validation')\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, (3,3), activation='relu', input_shape=(64,64,3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_gen, validation_data=val_gen, epochs=3)"
      ],
      "metadata": {
        "id": "rM2wG-ZNfteg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q.10 You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into “Normal” and “Pneumonia” categories. Describe your end-to-end approach-from data preparation and model training to deploying the model as a web app using Streamlit.**\n",
        "\n",
        "**→** The end-to-end approach for Chest X-ray classification are as follows:\n",
        "\n",
        "**1.** **Data Preparation:**\n",
        "- Collect dataset (Normal vs Pneumonia).\n",
        "- Preprocess (resize to 224×224, normalization).\n",
        "- Split into training/validation/test.\n",
        "\n",
        "**2. Model Training:**\n",
        "- Use CNN (e.g., ResNet, VGG, or custom CNN).\n",
        "- Activation: ReLU in hidden layers, Sigmoid in output.\n",
        "- Loss: Binary Cross-Entropy.\n",
        "- Optimizer: Adam.\n",
        "- Techniques: Data Augmentation, Early Stopping, Dropout.\n",
        "\n",
        "**3. Deployment with Streamlit:**"
      ],
      "metadata": {
        "id": "_j8ybItOVqNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "model = load_model(\"xray_model.h5\")\n",
        "\n",
        "st.title(\"Chest X-ray Classification\")\n",
        "uploaded_file = st.file_uploader(\"Upload X-ray\", type=[\"jpg\",\"png\"])\n",
        "if uploaded_file:\n",
        "    img = image.load_img(uploaded_file, target_size=(224,224))\n",
        "    x = image.img_to_array(img)/255.0\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    pred = model.predict(x)[0][0]\n",
        "    st.write(\"Prediction: Pneumonia\" if pred>0.5 else \"Normal\")"
      ],
      "metadata": {
        "id": "mJGH6H6DZ0xW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}