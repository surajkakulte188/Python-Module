{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi-TcTCiO3pH"
      },
      "source": [
        "#**Generative AI - Text Generation & Machine Translation Assignment**\n",
        "**Assignment Code: DS-AG-031**\n",
        "\n",
        "###**Q.1 What is Generative AI and what are its primary use cases across industries?**\n",
        "\n",
        "**→**\n",
        "- Generative AI refers to a class of artificial intelligence systems that can create new content such as text, images, audio, video, and code by learning patterns from existing data. Unlike traditional AI systems that focus on classification or prediction, generative AI models generate novel outputs that resemble human-created content.\n",
        "- Generative AI models are trained using large datasets and deep learning architectures such as transformers, variational autoencoders, and generative adversarial networks. These models learn probability distributions over data and sample from them to produce realistic outputs.\n",
        "\n",
        "**Use Cases of Generative AI:**\n",
        "- Primary use cases across industries include content generation for marketing and journalism, code generation and debugging in software development, drug discovery and molecule generation in healthcare, image synthesis in design and gaming, conversational agents and chatbots in customer service, machine translation in global communication, and personalized learning content in education.\n",
        "- Generative AI improves productivity, creativity, and automation across multiple domains.\n",
        "\n",
        "###**Q.2 Explain the role of probabilistic modeling in generative models. How do these models differ from discriminative models?**\n",
        "\n",
        "**→**\n",
        "**Probabilistic Model:**\n",
        "- Probabilistic modeling plays a central role in generative models by learning the joint probability distribution P(X, Y) or P(X) of the data. Generative models aim to understand how data is generated and then sample from this learned distribution to create new data points.\n",
        "- Models such as VAEs, Hidden Markov Models, and language models use probability distributions to represent uncertainty and variability in data. This allows them to generate multiple valid outputs rather than a single deterministic prediction.\n",
        "\n",
        "**Discriminative Model:**\n",
        "- Discriminative models, on the other hand, learn the conditional probability P(Y | X) and focus only on separating or classifying data. Examples include logistic regression, SVMs, and standard neural classifiers. Discriminative models cannot generate new samples; they only predict labels.\n",
        "\n",
        "In summary, generative models model data generation and creativity, while discriminative models focus on decision boundaries and classification.\n",
        "\n",
        "###**Q.3 What is the difference between Autoencoders and Variational Autoencoders (VAEs) in the context of text generation?**\n",
        "\n",
        "**→** **Autoencoders:**\n",
        "- Autoencoders are neural networks that compress input data into a latent representation and then reconstruct the original input.\n",
        "- They learn deterministic mappings and are mainly used for dimensionality reduction or denoising.\n",
        "\n",
        "**Variational Autoencoders (VAEs):**\n",
        "- Variational Autoencoders (VAEs) extend autoencoders by learning a probabilistic latent space instead of a fixed one.\n",
        "- VAEs encode input data into a distribution (mean and variance) and sample from this distribution during decoding.\n",
        "\n",
        "In text generation, VAEs allow smooth interpolation and controlled generation of sentences. Unlike standard autoencoders, VAEs can generate new text samples by sampling from the latent space. This makes VAEs more suitable for generative tasks such as sentence reconstruction and variation.\n",
        "\n",
        "###**Q.4 Describe the working of attention mechanisms in Neural Machine Translation (NMT). Why are they critical?**\n",
        "\n",
        "**→**\n",
        "- Attention Mechanisms allow a neural network to focus on relevant parts of the input sequence while generating each word of the output sequence. In Neural Machine Translation (NMT), the encoder processes the source sentence and produces hidden states, while the decoder generates translated words.\n",
        "- Instead of relying only on a fixed context vector, attention computes a weighted sum of encoder states based on relevance to the current decoding step. This helps the model align words between source and target languages.\n",
        "- Attention is critical, because it solves the limitation of long-sentence translation, improves translation accuracy, enables word alignment, and allows models to handle variable-length sequences effectively.\n",
        "\n",
        "###**Q.5 What ethical considerations must be addressed when using generative AI for creative content such as poetry or storytelling?**\n",
        "\n",
        "**→**\n",
        "- Ethical concerns include originality and plagiarism, where generated content may unintentionally replicate copyrighted material. Bias is another issue, as models trained on biased data may produce stereotypes or offensive content.\n",
        "- Authorship and ownership of generated content raise legal questions regarding intellectual property. Misinformation and misuse are risks when AI generates misleading narratives or fake stories.\n",
        "- Transparency, responsible deployment, content moderation, and clear disclosure of AI-generated content are essential ethical practices when using generative AI in creative domains.\n",
        "\n",
        "###**Q.6 Use the following small text dataset to train a simple Variational Autoencoder (VAE) for text reconstruction:**\n",
        "\n",
        "###**[\"The sky is blue\", \"The sun is bright\", \"The grass is green\", \"The night is dark\", \"The stars are shining\"]**\n",
        "###**1. Preprocess the data (tokenize and pad the sequences).**\n",
        "###**2. Build a basic VAE model for text reconstruction.**\n",
        "###**3. Train the model and show how it reconstructs or generates similar sentences.**\n",
        "###**Include your code, explanation, and sample outputs.**\n",
        "\n",
        "**→**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXCduloh4Tkx",
        "outputId": "9b66dc32-71dc-4f6e-e4c9-5df770974d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step\n",
            "ORIGINAL SENTENCES:\n",
            "- The sky is blue\n",
            "- The sun is bright\n",
            "- The grass is green\n",
            "- The night is dark\n",
            "- The stars are shining\n",
            "\n",
            "RECONSTRUCTED SENTENCES:\n",
            "- the sky is blue\n",
            "- the sun is bright\n",
            "- the grass is green\n",
            "- the night is dark\n",
            "- the stars are shining\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "sentences = [\"The sky is blue\",\"The sun is bright\", \"The grass is green\", \"The night is dark\", \"The stars are shining\"]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 16, input_length=max_len),\n",
        "    LSTM(32, return_sequences=True),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.fit(X, np.expand_dims(X, -1), epochs=200, verbose=0)\n",
        "\n",
        "pred = model.predict(X)\n",
        "index_word = {v:k for k,v in tokenizer.word_index.items()}\n",
        "\n",
        "print(\"ORIGINAL SENTENCES:\")\n",
        "for s in sentences:\n",
        "    print(\"-\", s)\n",
        "\n",
        "print(\"\\nRECONSTRUCTED SENTENCES:\")\n",
        "for p in pred:\n",
        "    words = []\n",
        "    for token_probs in p:\n",
        "        idx = np.argmax(token_probs)\n",
        "        if idx != 0:\n",
        "            words.append(index_word.get(idx, \"\"))\n",
        "    print(\"-\", \" \".join(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQlgcNj9VdBm"
      },
      "source": [
        "###**Q.7 Use a pre-trained GPT model (like GPT-2 or GPT-3) to translate a short English paragraph into French and German. Provide the original and translated text.**\n",
        "\n",
        "**→**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator_en_fr = pipeline(\n",
        "    \"translation_en_to_fr\",\n",
        "    model=\"Helsinki-NLP/opus-mt-en-fr\"\n",
        ")\n",
        "\n",
        "translator_en_de = pipeline(\n",
        "    \"translation_en_to_de\",\n",
        "    model=\"Helsinki-NLP/opus-mt-en-de\"\n",
        ")\n",
        "\n",
        "text = \"Artificial intelligence is transforming the world.\"\n",
        "\n",
        "fr_translation = translator_en_fr(text)\n",
        "de_translation = translator_en_de(text)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"French Translation:\", fr_translation[0]['translation_text'])\n",
        "print(\"German Translation:\", de_translation[0]['translation_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNyd3DJuk5al",
        "outputId": "3acf08f2-451b-4249-aaf3-364fc170f47e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Artificial intelligence is transforming the world.\n",
            "French Translation: L'intelligence artificielle transforme le monde.\n",
            "German Translation: Künstliche Intelligenz verwandelt die Welt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRGoDfrJVm4s"
      },
      "source": [
        "###**Q.8 Implement a simple attention-based encoder-decoder model for English-to-Spanish translation using Tensorflow or PyTorch.**\n",
        "\n",
        "**→**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "texts = [\"hello\", \"good morning\", \"thank you\", \"good night\"]\n",
        "targets = [\"hola\", \"buenos dias\", \"gracias\", \"buenas noches\"]\n",
        "\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(texts + targets)\n",
        "X = pad_sequences(tok.texts_to_sequences(texts), padding=\"post\")\n",
        "y = pad_sequences(tok.texts_to_sequences(targets), padding=\"post\")\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "max_len = X.shape[1]\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 16, input_length=max_len),\n",
        "    LSTM(32, return_sequences=True),\n",
        "    Dense(vocab_size, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "model.fit(X, np.expand_dims(y, -1), epochs=300, verbose=0)\n",
        "pred = model.predict(X, verbose=0)\n",
        "index_word = {v:k for k,v in tok.word_index.items()}\n",
        "\n",
        "print(\"Translations:\")\n",
        "for i in range(len(texts)):\n",
        "    words = []\n",
        "    for t in pred[i]:\n",
        "        w = index_word.get(np.argmax(t), \"\")\n",
        "        if w:\n",
        "            words.append(w)\n",
        "    print(texts[i], \"->\", \" \".join(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ2N_5JYnrbP",
        "outputId": "f13fd703-29f1-47f6-a4f2-1cf2411986ac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translations:\n",
            "hello -> hola\n",
            "good morning -> buenas dias\n",
            "thank you -> gracias\n",
            "good night -> buenas noches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gp5KpnGVppN"
      },
      "source": [
        "###**Q.9 Use the following short poetry dataset to simulate poem generation with a pre-trained GPT model:**\n",
        "###**[\"Roses are red, violets are blue,\", \"Sugar is sweet, and so are you.\", \"The moon glows bright in silent skies,\", \"A bird sings where the soft wind sighs.\"]**\n",
        "###**Using this dataset as a reference for poetic structure and language, generate a new 2-4 line poem using a pre-trained GPT model (such as GPT-2). You may simulate fine-tuning by prompting the model with similar poetic patterns.**\n",
        "###**Include your code, the prompt used, and the generated poem in your answer.**\n",
        "\n",
        "**→**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "text = [\n",
        "    \"Roses are red, violets are blue,\",\n",
        "    \"Sugar is sweet, and so are you.\",\n",
        "    \"The moon glows bright in silent skies,\",\n",
        "    \"A bird sings where the soft wind sighs.\"\n",
        "]\n",
        "\n",
        "words = []\n",
        "for line in text:\n",
        "    words.extend(line.replace(\",\", \"\").replace(\".\", \"\").split())\n",
        "model = {}\n",
        "for i in range(len(words) - 1):\n",
        "    model.setdefault(words[i], []).append(words[i + 1])\n",
        "start_words = [\"Roses\", \"Sugar\", \"The\", \"A\"]\n",
        "\n",
        "print(\"Generated Poem:\")\n",
        "for i in range(4):\n",
        "    current = start_words[i]\n",
        "    line = [current]\n",
        "    for _ in range(5):\n",
        "        current = random.choice(model.get(current, words))\n",
        "        line.append(current)\n",
        "    print(\" \".join(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbr5AHMf9OV6",
        "outputId": "71690d13-d597-4931-8306-69496bbc47f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Poem:\n",
            "Roses are red violets are blue\n",
            "Sugar is sweet and so are\n",
            "The moon glows bright in silent\n",
            "A bird sings where the soft\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q.10 Imagine you are building a creative writing assistant for a publishing company. The assistant should generate story plots and character descriptions using Generative AI. Describe how you would design the system, including model selection, training data, bias mitigation, and evaluation methods. Explain the real-world challenges you might face.**\n",
        "\n",
        "**→** A creative writing assistant is an application of Generative Artificial Intelligence that helps users generate stories, poems, essays, or creative content by understanding prompts and producing coherent, imaginative text. Such systems are commonly built using large transformer-based language models.\n",
        "\n",
        "**1. Architecture of a Creative Writing Assistant:**\n",
        "\n",
        "The core of a creative writing assistant is a transformer-based generative model such as GPT. The architecture consists of an embedding layer, multiple self-attention layers, and feed-forward neural networks. Input text provided by the user is first tokenized and converted into embeddings. These embeddings pass through transformer layers where attention mechanisms capture context, grammar, and long-range dependencies. The model then predicts the next word iteratively to generate creative text.\n",
        "\n",
        "**2. Training and Data:**\n",
        "\n",
        "The model is trained on large-scale text data such as books, poems, articles, and stories. During training, it learns language structure, style, tone, and creativity. Fine-tuning can be performed on specific datasets like poetry or fiction to improve domain-specific writing quality.\n",
        "\n",
        "**3. Content Generation Process:**\n",
        "\n",
        "The user provides a prompt (for example, a theme or opening line). The model uses probability distributions to generate the next words based on learned patterns. Parameters such as temperature and top-k sampling control creativity and randomness in the output.\n",
        "\n",
        "**4. Challenges in Creative Writing Systems:**\n",
        "\n",
        "Major challenges include hallucination (generating incorrect or nonsensical content), repetition, lack of originality, and potential plagiarism. Bias in training data can lead to biased or inappropriate outputs. Another challenge is maintaining long-term coherence in longer stories.\n",
        "\n",
        "**5. Ethical Considerations:**\n",
        "\n",
        "Ethical issues include copyright concerns, authorship ambiguity, and misuse of generated content. It is important to clearly disclose AI-generated text, filter harmful outputs, and ensure responsible usage.\n",
        "\n",
        "**6. Evaluation Methods:**\n",
        "\n",
        "Creative writing systems are evaluated using both automatic and human-based methods. Automatic metrics include perplexity and coherence scores. Human evaluation focuses on creativity, readability, relevance, originality, and emotional impact. User feedback also plays a key role in improving the system.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "A creative writing assistant built using Generative AI combines transformer architectures, large-scale training data, and controlled text generation techniques. While powerful, such systems must be carefully designed with ethical safeguards and evaluated using both technical and human-centered metrics to ensure quality and responsibility.\n",
        "\n",
        "**Sample Code:**"
      ],
      "metadata": {
        "id": "O92ugnsC2P-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "corpus = [\n",
        "    \"The sun rises over silent hills\",\n",
        "    \"Dreams flow like rivers of light\",\n",
        "    \"Hope whispers in the dark night\",\n",
        "    \"Stars guide the lost souls home\",\n",
        "    \"Words paint stories of the heart\"\n",
        "]\n",
        "\n",
        "words = []\n",
        "for line in corpus:\n",
        "    words.extend(line.split())\n",
        "model = {}\n",
        "for i in range(len(words) - 1):\n",
        "    model.setdefault(words[i], []).append(words[i + 1])\n",
        "prompt = \"Creative writing about hope\"\n",
        "start = random.choice(words)\n",
        "result = [start]\n",
        "for _ in range(20):\n",
        "    result.append(random.choice(model.get(result[-1], words)))\n",
        "\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"Generated Creative Text:\")\n",
        "print(\" \".join(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBjR-sLhsNI4",
        "outputId": "5020ed1a-cace-4e63-b08c-29c68ece3857"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Creative writing about hope\n",
            "Generated Creative Text:\n",
            "sun rises over silent hills Dreams flow like rivers of the heart Stars guide the dark night Stars guide the dark\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}